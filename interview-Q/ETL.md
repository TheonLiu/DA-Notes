## 数据抽取
> [来源](https://www.zhihu.com/question/22897990/answer/660287922)  

分类：全量抽取和增量抽取多种方式
### 增量抽取方式：
时间戳方式：
日志表方式：分析数据库自身在线日志判断变化数据。对源数据表进行insert、update或 delete
全表比对方式：
触发器方式：


## 数据转换
数据转换,空值处理,规范化数据格式,拆分数据,验证数据合法性,数据替换,实现数据规则过滤,数据排序,数据类型统一转换
* 不一致数据转换
* 数据粒度的转换
* 商务规则的计算

## 提取有用的特征
数字特征，分类特征，文本特征，其他特征(地理位置)
文本特征:
分词(tokenization)：首先会应用某些分词方法来将文本分隔为一个由词（一般如单词，数字等）组成的集合．可用的方法如空白分隔法．这种方法在空白处对文本分隔并可能还删除其他如标点符号和其他非字母或数字字符．
删除停用词(stop words removal)：之后，它通常会删除常见的单词，比如the, and和but（这些词被称作停用词）.
提取词干(stemming)：下一步则是词干的提取．这是指将各个词简化为其基本的形式或者干词．常见的例子如复数变为单数．提取的方法有很多种，文本处理算法库中常常会包括多种词干提取方法．
向量化(vectorization)：

## 正则化特征
* 使用MLlib实现特征标准化
StandardScaler，用于标准正态变换；以及Normalizer，提供了我们之前处理示例代码中的向量标准化功能。

### 数据质量
正确性(Accuracy):数据是否正确体现在现实或可证实的来源
完整性(Integrity):数据之间的参照完整性是否存在或一致
致性(Consistency):数据是否被一致的定义或理解
完备性(Completeness):所有需要的数据是否都存在
有效性(Validity):数据是否在企业定义的可接受的范围之内
时效性(Timeliness):数据在需要的时间是否有效
可获取性(Accessbility):数据是否易于获取、易于理解和易于使用

## 数据清洗
> [来源1](https://www.zhihu.com/question/22077960)
## 步骤
选择子集 -> 列名重命名 -> 删除重复值 -> 缺失值处理 -> 一致化处理 -> 数据排序处理 -> 异常值处理
### 大概工作
纠正错误、删除重复项、统一规格、修正逻辑、转换构造、数据压缩、补足残缺/空值、丢弃数据/变量
### 纠正错误
数据值错误；数据类型错误；数据编码错误；数据格式错误；数据异常错误；依赖冲突；多值错误
### 缺失值处理
* 直接删除（缺失值数据量较小）
* 均值，中位数，众数（不随机数据产生偏差，正常分布用均值，数据倾斜用中位数）
* 以不同指标的计算结果填充缺失值。(年龄和身份证的关系)
* 插补法：随机抽取某样本；多重插补法，利用蒙特卡洛方法生成多个完整的数据集；在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本）；拉格朗日差值法和牛顿插值法
* 建模法：回归，决策树归纳。(eg:利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值)
### 格式内容清洗
* 时间、日期、数值、全半角等显示格式不一致
* 内容中有不该存在的字符
* 内容与该字段应有内容不符
### 逻辑清洗
去重，不合理值，矛盾内容
### 异常值处理
判断离群点：
* 简单统计分析：pandas的describe()(eg:最大最小值)
* 如果数据服从正态分布，与平均值的偏差超过3倍标准差的值。不服从正态分布，也可以用远离平均值的多少倍标准差
* 箱型图：小于QL01.5IQR或大于OU-1.5IQR。QL：下四分位数。QU：上四分位数。IQR：四分位数间距(QU与下四分位数QL的差值)
* 基于模型检测:簇的集合;回归模型
* 距离：O(m2)大数据不适用
* 密度：一个点的局部密度显著低于它的大部分近邻时
* 聚类：

处理方法：
* 删除异常值---明显看出是异常且数量较少可以直接删除
* 不处理---如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的。
* 平均值替代----损失信息小，简单高效。
* 视为缺失值----可以按照处理缺失值的方法来处理
### 去重处理：
### 噪音处理：
分箱法：考察数据的“近邻”，滑有序数据值
回归法：用一个函数拟合数据来光滑数据

## 你觉得Spark和Hadoop的区别是什么,请简要说一说？
Hadoop适合离线分析，是批处理；Spark适合实时分析，是近实时流，微批处理。

