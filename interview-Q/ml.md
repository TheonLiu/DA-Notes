<!-- TOC -->

- [六：机器学习：](#六机器学习)

<!-- /TOC -->
# 六：机器学习：
1.	完整解释PCA。 PCA缺点：高维数据能适用PCA吗？类别变量onehot能用PCA吗？
高维度特征数据预处理方法。原有n维特征的基础上重新构造出来的k维特征。保留相互正交的维度，去除方差几乎为0的维度
2.	Kmeans，KNN区别
Kmeans：k-均值聚类分析
KNN: 分类算法，求最近邻
3.	决策树和随机森林优缺点
决策树：
容易过拟合，导致泛化能力不强。Sl：设置节点最少样本数量和限制决策树深度。
有些比较复杂的关系，决策树很难学习，比如异或。Sl：用神经网络
特征的样本比例过大，生成决策树容易偏向于这些特征。SL：调节样本权重
随机森林：
1）	【输入数据】是随机的从整体的训练数据中选取一部分作为一棵决策树的构建，而且是有放回的选取
2）	每棵决策树的构建所需的【特征】是从整体的特征集随机的选取的;
两个随机性的引入，使得随机森林不容易陷入过拟合
处理很高维度（feature很多）的数据，并且不用做特征选择
能够检测到feature间的互相影响
4.	偏态分布怎么处理
5.	特征工程怎么做的
分解类别属性（01二元属性），特征分区（标量，比如年龄）交叉特征（组合的特征要比单个特征更好时），特征选择（修剪特征来达到减少噪声和冗余），特征缩放（岭回归） 数据标准化，特征提取（降维）
6.	Ensemble：将多个分类方法聚集在一起，以提高分类的准确率
分类：Bagging（随机森林），Boosting（AdaBoost，XGboost，GBDT），Stacking
GBDT: 每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上建立一个新的模型。
Bagging：有放回选取，训练集之间是独立。并行
Boosting：训练集不变，分类器中的权重发生变化，权值是根据上一轮的分类结果错误率进行调整。顺序
GBDT：将目标函数泰勒展开到一阶，新的基模型寻找新的拟合标签。xgboost加入了和叶子权重的L2正则化项。自动处理缺失值特征的策略
Xgboost：将目标函数泰勒展开到了二阶，
怎么提高ensemble的表现：选取分裂点，分裂位置
基本精确的贪心算法，近似算法，带权重的分位数草图
7.	模型评估指标的选择
分类任务：准确率和错误率
查准率和查全率：P=TP/TP+FP，R=TP/TP+FN
8.	ROC曲线 与 AUC怎么算
ROC：正负样本的分布变化的时候，ROC曲线能够保持不变
     纵坐标：真正率 TP/TP+FN
     横坐标：假正率FP/FP+TN
AUC：ROC下方面积大小
9.	logistics regression的损失函数方程
10.	得简单模型和复杂模型区别在哪

11.	CNN模型结构，卷积层的作用
卷积层：提取特征
池化层：：对输入的特征图进行压缩，提取主要特征
全连接层：分类
12.	如何看待基础性的工作，如数据清洗、数据抽取这些？
13.	如果取出的数据中存在NULL，该如何做
1）	平均值，中位数，众数，随机数
2）	将变量映射到高维空间（2个值->3个值，男女null）。连续：平滑处理
3）	根据欧式距离或Pearson相似度，来确定和缺失数据样本最近的K个样本，将这K个样本的相关feature加权平均来估计该样本的缺失数据。
14.	如果因为写的代码出现问题，导致取数出现NULL，该如何做
5.	数据清洗（完整性，唯一性，权威性，合法性，一致性）
唯一性：去重。合法性：人工处理，设置警告。