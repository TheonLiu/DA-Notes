## 支付宝实习：
流量维度-星巴克100杯业务指标，来源于人。多少人打开微信
业务维度交易层面-星巴克卖了多少咖啡，阿里腾讯下单量
用户层-用户画像，拼多多，三四线。京东一二线男性。天猫一二线女性。
产品层-平台分析，京东卖电脑（高级的，外星人，购买率, 销售走势）
星巴克-冬天-热的拿铁，中杯销量第一名
（推荐产品的产品库的分析，销量排名，对产品分析）
核心：数据清洗的学习，指标的搭建，ab测试。

订单为主体的，用户
1产品转换变低-告诉产品经理-快速定位-做了个预警
数据监控-数据分析
2增长实验
产品经理-提升首页的弹窗的点击率-abtest（实验组对照组）eg：某一个按/UI变更提高转换率-3种类型按钮 点击率高17%，提高用户整体的点击意愿
收获：不仅被动分析数据，主动利用数据产品经理达成合作，提升流量价值利用率
影响逾期状态，网上找报告。用户基本信息，设计年龄 婚姻 性别 学历 所处地区。第二各维度：历史行为数据。
补充：
Ab测试，前期假设：用户打开支付宝后，弹窗不够好，只有1%，是否通过更好ui或者按钮文案做差异化。三四线小镇青年，年纪大一点，对弹窗文案样式不同需求。做基础分群，消费金融，理财，对ui会不同样式。1000w人分出10组，其中两实验组比对照组要高。
目标头：资金产品页面，页面测试，只有测试内容不同。（协助学姐，对接产品团队）
Sql：数据获取和筛查。数据表表头，给出想要相应提取的内容（github）。
Sql的etl处理：订单层面数据重构，转换为用户数据，聚合成以天为单位。rfm变量，一段时间购买金额转换为etl操作。做了些数据加工。作用：每天自动跑批。了解常见的聚合函数，表和表关联方式。脏数据，无效订单过滤掉。
上下游依赖东西。
Python：爬取应用商店里支付宝的评论。落款id号，日期，内容，相应版本号，应用商店名称。
财富管理：订单层面转换为用户维度主题

## 步态识别：
20w行数据，多分类，人物识别。Cnn池化层。如何设计cnn，设计过程遇到什么问题。通过什么样方法优化模型。
训练精度不是很高，60%提高到80%。数据工程，feature100个，特征提取，特征加工，提升到1000个。不同模型尝试，cnn效果一般，换了其他更复杂模型。
传统机器学习，深度学习，发现最好模型。

## Kaggle：
任务：2400w个节点对，代表了互相关注信息。然后让预测其中100个节点对有没有互相关注
数据采样：2400w节点对。5000输出0，5000输出1。测试节点全部包括，测试集相连的点全部包括。0节点随机选，检查是否存在
特征工程：
最开始：
node2vec - graph embedding algorithms
Node2vec 用随机游走，创造节点向量。
过拟合：本地auc93，kaggle 65.训练集太小太离散了。之后舍弃node2vec
特征工程的资料：
CN共同的邻居（交集）。邻居局部重合度。交集除以并集（兼顾了邻居的重合程度并且通过标准化降低邻居过多/少的极值节点带来的偏差），邻居相乘（富者越富穷者越穷）。共13个feature。
Simrank，全局重合度：kartz index

模型选取：避免过拟合：logistics regression，随机森林，gbdt。结果 Gbdt比较好
Kaggle的auc依然高于本地，因为随机生成的未连接的图和真实不同。
LR虽然模型简单，结果不好。边界非线性。但是可以通过权重过滤一些特征
随机森林：决策树的叶节点包含多个导致数据集拟合不足的标签。
Gbdt：最好。但有点过拟合->调整树的数量，random iterative sampling
总结：解决过拟合，（不需要scale）
1如何采样，太小会过拟合，太多会难以计算。所以选取5000
2选取node2vec可以避免过拟合，但是效果不好，原因如上。
3树结构解决过拟合，random forest，最好的是gbdt，拟合略弱。以后会尝试神经网络，正则化等避免过拟合



逻辑回归，树模型，xgboost，深度学习。
Xgboost好多树叠在一块，减少overfitting。变量重要性的筛选。到底决定相似度的是哪些。训练速度比较慢。爬取额外数据

分类模型：回归模型，分类模型。
带参的：多元线性回归，逻辑回归，cnn。
树模型：xgboost，随机森林，jbs
非监督学习：kmeans聚类

工具：python, R
数据可视化：tableau，呈现一个数据库
跟excel比，饼图，柱状图。可以数据探索，可以下钻。
Eg：股市下跌，某一段下跌很惨，看具体哪些股票下跌
相关性分析，聚类分析

总结：
数据加工，数据建模，指标分析，可视化
