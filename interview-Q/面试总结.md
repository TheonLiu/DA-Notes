## 支付宝实习：
### A/B实验
增长实验
**AB test**：用户打开资金产品页面，页面测试，只有测试内容不同。是否通过更好ui或者按钮文案做差异化。三四线小镇青年，年纪大一点，对弹窗文案样式不同需求。做基础分群，消费金融，理财，对ui会不同样式。1000w人分出10组，其中两实验组比对照组要高。
目标头：资金产品页面，页面测试，只有测试内容不同。（协助学姐，对接产品团队）
1. **指标**：CTR
2. **变体**：新的推荐策略
3. **假设**：新的推荐策略可以带来更多的用户点击
    H0：功能优化后与优化前没有差异，u2-u1=0
    H1：功能优化后于优化前有差异，u2-u1≠0
    u为点击率
4. **收集数据**：以下B组数据为我们想验证的新的策略结果数据，A组数据为旧的策略结果数据。实验组7天点击率，对照组7天点击率。
5. **样本量**的四个影响因素：
    * 显著性水平（α）：显著性水平越低，对实验结果的要求也就越高，越需要更大的样本量来确保精度
    * 统计功效（1 – β）：统计功效意味着避免犯二类错误的概率，这个值越大，需要的样本量也越大
    * 原始版本转换率,最小提升比例
    * （均值差异（μA-μB）：如果两个版本的均值差别巨大，也不太需要多少样本，就能达到统计显著）
    * （标准差（σ）：标准差越小，代表两组差异的趋势越稳定。越容易观测到显著的统计结果）
6. **选择检验**：同统计日之间是有随机波动的差异，而且实验组和对照组流量相等且随机，可以认为样本来自同一个总体。实验组是对同一天的对照组进行优化的结果，所以可以采用相关样本t检验
确定显著性水平：a=0.05
6. **计算统计量**：相关样本t检验是以每一组数据的差值作为检验的，所以以点击率差作检验，确定七组差值的均值md，求出样本方差开根号，代入t分数
7. **查表**
8. **注意事项**
   * 时间
   测试时间=最小测试样本量/每日流量
   用户行为周期（工作日和周末）
   用户适应周期
   * 样本质量
    检验样本有效性
    AA测试：旧版本90%，新版本10%。旧版本90%切分出两个10%，查看是否有显著性差异。如果有显著性差异说明有问题。


#### 定义
在同一时间维度，分别让组成成分相似的访客群组访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用
#### 好处
消除不同意见，提高团队效率（选择不同UI界面）。
通过对比实验，验证问题原因（漏斗模型无效点击）。
建立科学运营优化体系，避免过度依赖人，降低人为风险，有效知识沉淀
#### 如何分组
假设Group1上的实验结果为 r1, Group2上的实验结果为r2，则AB测试的差异是 r = r1 − r2，r是依赖于测试样本的随机变量，应该满足：
**无偏性**：假设在1%流量上某功能可以提高10%的点击率，那么在全量上也应该大约提高10%
**低方差**。r rr是一个随机变量，方差越小，可靠性越高。
用户基本信息，设计年龄 婚姻 性别 学历 所处地区。第二各维度：历史行为数据
#### 假设检验:
H0：假定总体没有显著差异，或是H0落在接收域
H1：题目中希望证明成功的假设
最小化的都是 Type 1 Error，即尽量避免在H0实际正确的时候拒绝掉H1
置信区间：有95%的机会，真实值落在我们的这个置信区间里
置信度：
**P值的含义**：在假设原假设（H0）正确时，出现现状或更差的情况的概率。对于观察到的采样，能获得的最小的显著性水平，就是p值。
**F检验**：两个卡方分布相处
**T检验**：主要用于样本含量较小（例如n < 30），总体标准差σ未知的正态分布，比较两个平均数的差异是否显著。（自由度 1的卡方/自由度为n-1的卡方分布）
**Z检验**：一般用于大样本平均值差异性检验的方法。标准正态分布的理论来推断差异发生的概率，从而比较两个平均数的差异是否显著
**卡方检验**: n个标准正态分布的随机变量的平方和构成一新的随机变量
**中心极限定理**：不管样本总体服从什么分布，当样本数量足够大时，样本的均值以正态分布的形式围绕总体均值波动
**两类错误**
第一类错误：原假设是正确的，却拒绝了原假设。(错杀好人)
第二类错误：原假设是错误的，却没有拒绝原假设。(放走坏人)
![统计量](https://pic1.zhimg.com/v2-97e2cd6235a3733611d65635c849f730_r.jpg)
![Z](https://pic4.zhimg.com/80/v2-fc83557e85fb7e827337e64eb06be347_1440w.jpg)

### ETL处理：
#### 数据清洗
删除重复值 -> 缺失值处理 -> 一致化处理 -> 数据排序处理 -> 异常值处理
#### 数据转换
数据转换,空值处理,规范化数据格式,拆分数据,验证数据合法性,数据替换,实现数据规则过滤,数据排序,数据类型统一转换
* 不一致数据转换
* 数据粒度的转换
* 商务规则的计算
* 
**提取有用的特征**
数字特征，分类特征，文本特征，其他特征(地理位置)
文本特征:
分词(tokenization)：首先会应用某些分词方法来将文本分隔为一个由词（一般如单词，数字等）组成的集合．可用的方法如空白分隔法．这种方法在空白处对文本分隔并可能还删除其他如标点符号和其他非字母或数字字符．
删除停用词(stop words removal)：之后，它通常会删除常见的单词，比如the, and和but（这些词被称作停用词）.
提取词干(stemming)：下一步则是词干的提取．这是指将各个词简化为其基本的形式或者干词．常见的例子如复数变为单数．提取的方法有很多种，文本处理算法库中常常会包括多种词干提取方法．
向量化(vectorization)：
**正则化特征**
* 使用MLlib实现特征标准化
StandardScaler，用于标准正态变换；以及Normalizer，提供了我们之前处理示例代码中的向量标准化功能。
### 数据转换：
Sql：数据获取和筛查。数据表表头，给出想要相应提取的内容（github）。
Sql的etl处理：订单层面数据重构，转换为用户数据，聚合成以天为单位。rfm变量，一段时间购买金额转换为etl操作。做了些数据加工。作用：每天自动跑批。了解常见的聚合函数，表和表关联方式。脏数据，无效订单过滤掉。
上下游依赖东西。
Python：爬取应用商店里支付宝的评论。落款id号，日期，内容，相应版本号，应用商店名称。
财富管理：订单层面转换为用户维度主题
### 收获：
流量维度-星巴克100杯业务指标，来源于人。多少人打开微信
业务维度交易层面-星巴克卖了多少咖啡，阿里腾讯下单量
用户层-用户画像，拼多多，三四线。京东一二线男性。天猫一二线女性。
产品层-平台分析，京东卖电脑（高级的，外星人，购买率, 销售走势）
星巴克-冬天-热的拿铁，中杯销量第一名
（推荐产品的产品库的分析，销量排名，对产品分析）
核心：数据清洗的学习，指标的搭建，ab测试。

****
## Kaggle：
### 任务
2400w个节点对，代表了互相关注信息。然后让预测其中100个节点对有没有互相关注
### 流程
#### 数据采样
2400w节点对。5000输出0，5000输出1。测试节点全部包括，测试集相连的点全部包括。0节点随机选，检查是否存在
#### 特征工程：
* 最开始：node2vec - graph embedding algorithms
**Node2vec** 
[介绍](https://zhuanlan.zhihu.com/p/46344860)
**概括**:用随机游走（综合了DFS和BFS）p，q控制概率，创造节点向量。
**适用情况**：网络节点的分类，链接预测
**功能**：提出了一个有效的、可扩展的表示学习算法，可以体现网络特征和节点邻居特征。
**两个假设**：条件独立，特征空间的对称性
**损失函数**：$\max _{f} \sum_{u \in V} \log \operatorname{Pr}\left(N_{s}(u) \mid f(u)\right)$
**随机游走**: 广度优先（节点邻居的微观属性），深度优先（节点邻居的宏观属性）
**结果**：过拟合，本地auc93，kaggle 65.训练集太小太离散了。之后舍弃node2vec
* PCA涉及到矩阵分解，运算量大
* **手动选取特征工程**：
CN共同的邻居（交集）。邻居局部重合度。交集除以并集（兼顾了邻居的重合程度并且通过标准化降低邻居过多/少的极值节点带来的偏差），邻居相乘（富者越富穷者越穷）。共13个feature。
Simrank，全局重合度：kartz index
#### 模型选取
**避免过拟合**
**尝试**：logistics regression，随机森林，gbdt。
**结果** Gbdt比较好
**不足**：Kaggle的auc依然高于本地，因为随机生成的未连接的图和真实不同。
**LR**：虽然模型简单，结果不好。边界非线性。但是可以通过权重过滤一些特征
**随机森林**：决策树的叶节点包含多个导致数据集拟合不足的标签。
**Gbdt**：最好。但有点过拟合->调整树的数量，random iterative sampling
**总结**：解决过拟合，（不需要scale）
1) 如何采样，太小会过拟合，太多会难以计算。所以选取5000
2) 选取node2vec可以避免过拟合，但是效果不好，原因如上。
node2vec:：生成随机游走，对随机游走采样得到（节点，上下文）的组合，然后用处理词向量的方法对这样的组合建模得到网络节点的表示
3) 树结构解决过拟合，random forest，最好的是gbdt，拟合略弱。以后会尝试神经网络，正则化等避免过拟合
****
## 步态识别：
20w行数据，多分类，人物识别。Cnn池化层。如何设计cnn，设计过程遇到什么问题。通过什么样方法优化模型。
训练精度不是很高，60%提高到80%。数据工程，feature100个，特征提取，特征加工，提升到1000个。不同模型尝试，cnn效果一般，换了其他更复杂模型。
传统机器学习，深度学习，发现最好模型。

#### 有哪些常用滤波器？
GaussianBlur; MediumBlur; MeanBlur

#### 边缘检测算子有哪些？
canny算子（一阶差分）; sobel算子（一阶差分）; laplacian算子（二阶差分）（先进行滤波，再进行边缘检测）

**canny算子**是怎么做的？
1) 将图像处理成灰度图
2) 进行高斯滤波
3) 分别计算x方向和y方向梯度，再用平方和开根号计算综合梯度
4) 用非极大值抑制去除非边缘像素    
5) 设置高阈值和低阈值：大于高阈值的被保留；低于低阈值的被抛弃；处于低阈值和高阈值之间的，如果和高阈值像素相连则被保留，否则被放弃。

**对比**：
canny算子检测的边缘很细，可能只有一个像素大小，没有强弱；sobel算子检测的边缘有强弱；laplacian算子对边缘很敏感，但可能会将不是边缘的也当成边缘。
详见：数字图像 - 边缘检测原理 - Sobel, Laplace, Canny算子

#### 霍夫变换是干嘛的？
主要是用来提取直线特征的。（首先要进行边缘检测得到二值化图像，再进行霍夫变换）
#### 提取任务
背景减除法；光流法；
#### 形态学操作
腐蚀：用周围的最大值填充
膨胀：用周围的最小值填充
    分别相当于“腐蚀”（减小）白色区域和“膨胀”（增大）白色区域。
开运算：先腐蚀后膨胀，把黑色区域连横一块。
闭运算：先膨胀后腐蚀，把黑色区域分成多块。
顶帽：原图与开运算之差。
黑帽：原图与闭运算之差。

#### 漫水填充
将一块连通区域填充为某种指定颜色。一般用于获取掩码或者某块区域用。

#### 图像直方图
直方图是一种数据统计的集合，统计数据可以是亮度、灰度、梯度任何能描述图像的特征。横轴表示强度值，纵轴表示该强度值区域下统计数据的数量。

#### mat申请矩阵后怎么释放内存？
mat.release()函数

##  RGB图像怎么转为灰度图？
opencv的cvtColor(src, COLOR_RGB2GRAY)函数，具体的算法是
$g r a y = 0.299 ∗ R + 0.587 ∗ G + 0.114 ∗ B gray = 0.299*R + 0.587*G + 0.114*Bgray=0.299∗R+0.587∗G+0.114∗B$
****

逻辑回归，树模型，xgboost，深度学习。
Xgboost好多树叠在一块，减少overfitting。变量重要性的筛选。到底决定相似度的是哪些。训练速度比较慢。爬取额外数据

分类模型：回归模型，分类模型。
带参的：多元线性回归，逻辑回归，cnn。
树模型：xgboost，随机森林，jbs
非监督学习：kmeans聚类

工具：python, R
数据可视化：tableau，呈现一个数据库
跟excel比，饼图，柱状图。可以数据探索，可以下钻。
Eg：股市下跌，某一段下跌很惨，看具体哪些股票下跌
相关性分析，聚类分析

总结：
数据加工，数据建模，指标分析，可视化
