<!-- TOC -->

- [深度学习](#深度学习)
  - [讲一讲faster_rcnn](#讲一讲faster_rcnn)
  - [你了解哪些激活函数？分别有什么用？](#你了解哪些激活函数分别有什么用)
  - [批梯度下降、随机梯度下降和mini-batch梯度下降的区别？](#批梯度下降随机梯度下降和mini-batch梯度下降的区别)
  - [CNN里有哪些常见的梯度下降方法？](#cnn里有哪些常见的梯度下降方法)
  - [池化层是怎样进行反向传播的？](#池化层是怎样进行反向传播的)
  - [深度学习不适合哪些应用场景？](#深度学习不适合哪些应用场景)
  - [浅层神经网络和深层神经网络的差别？](#浅层神经网络和深层神经网络的差别)
  - [什么是过拟合？防止过拟合的方法？](#什么是过拟合防止过拟合的方法)
  - [为什么负梯度方向是函数局部值最快的方向？](#为什么负梯度方向是函数局部值最快的方向)
  - [为什么图像处理中一般用最大池化而不用平均池化？](#为什么图像处理中一般用最大池化而不用平均池化)
  - [softmax loss和cross entropy的区别？](#softmax-loss和cross-entropy的区别)
  - [全连接层的作用](#全连接层的作用)
  - [训练不收敛（loss不下降）怎么办？](#训练不收敛loss不下降怎么办)
  - [怎么计算感受野（receptive field）？](#怎么计算感受野receptive-field)
  - [详细介绍一下Batch Normalization（怎么计算均值、方差，怎么反向传播等）](#详细介绍一下batch-normalization怎么计算均值方差怎么反向传播等)
  - [RNN和LSTM有什么不同？](#rnn和lstm有什么不同)
  - [为什么RNN不用Relu函数而要用tanh函数？](#为什么rnn不用relu函数而要用tanh函数)
  - [写一下卷积的实现](#写一下卷积的实现)
  - [空洞卷积的思想？或者说相比如标准卷积有什么改进？](#空洞卷积的思想或者说相比如标准卷积有什么改进)
  - [CNN模型结构，卷积层的作用](#cnn模型结构卷积层的作用)

<!-- /TOC -->
## 深度学习
### 讲一讲faster_rcnn
1.讲一讲faster_rcnn
### 你了解哪些激活函数？分别有什么用？
阶跃函数；sigmoid函数；tanh函数；ReLU函数；Softmax函数
### 批梯度下降、随机梯度下降和mini-batch梯度下降的区别？
**批梯度下降**是每次使用所有数据用于更新梯度，使得梯度总是朝着最小的方向更新，但数据量很大的时候更新速度太慢，而且容易陷入局部最优。
**随机梯度下降**是每次使用一条数据来更新梯度，在梯度更新过程中梯度可能上升也可能下降，但总的来说梯度还是朝着最低的方向前进；最后梯度会在极小值附近徘徊。随机梯度下降的梯度更新速度快于批梯度下降，且由于每次梯度的更新方向不确定，陷入局部最优的时候有可能能跳出该局部极小值。
**mini-batch**梯度下降介于批梯度下降和随机梯度下降之间，每次用若干条数据更新梯度。mini-batch梯度下降可以使用矩阵的方式来计算梯度，因此速度快于随机梯度下降，且同样具有跳出局部最优的特点。
### CNN里有哪些常见的梯度下降方法？
GradientDescent; Momentum; RMSprop; Adam
### 池化层是怎样进行反向传播的？
mean_pooling; max_pooling
### 深度学习不适合哪些应用场景？
* 日常生活的预测问题或生物学中的小样本问题：在这些问题中使用线性模型往往能比深度学习效果更好，而且速度更快。
* 需要知道变量与结果之间的关系，而不是仅仅给出一个预测值的时候。例如，医生想要知道人口变量对死亡率的影响，这个时候深度学习就不适合。
* 非结构化数据中深度学习不适合。
### 浅层神经网络和深层神经网络的差别？
神经网络中，权重参数是给数据做线性变换，而激活函数给数据带来的非线性变换。增加某一层神经元数量是在增加线性变换的复杂性，而增加网络层数是在增加非线性变换的复杂性。
理论上来说，浅层神经网络就能模拟任何函数，但需要巨大的数据量，而深层神经网络可以用更少的数据量来学习到更好的拟合
### 什么是过拟合？防止过拟合的方法？
* L2正则化: L2正则化只对W有影响，对b没有影响。
* L1正则化: L1正则化使得参数W在更新时向0靠近使得参数W具有稀疏性。而权重趋近0，也就相当于减小了网络复杂度，防止过拟合
* Dropout
Dropout在每次训练时，有一部分神经元不参与更新，而且每次不参与更新的神经元是随机的。随着训练的进行，每次用神经元就能拟合出较好的效果，少数拟合效果不好的也不会对结果造成太大的影响。
增大数据量
既然过拟合是学习到了部分数据集的特有特征，那么增大数据集就能有效的防止这种情况出现。
* Early stop
数据分为训练集、验证集和测试集，每个epoch后都用验证集验证一下，如果随着训练的进行训练集loss持续下降，而验证集loss先下降后上升，说明出现了过拟合，应该立即停止训练。
* Batch Normalization（下面另有BN的详细过程，见问题21）
BN的过程中，每次都是用一个mini_batch的数据来计算均值和反差，这与整体的均值和方差存在一定偏差，从而带来了随机噪声，起到了与Dropout类似的效果，从而减轻过拟合。
### 为什么负梯度方向是函数局部值最快的方向？
$|grad_f|*|l|*cos(θ)$最大（其中θ是l与$grad_f$的夹角），很明显θ = 0时该值最大，也就是说l方向就是$grad_f$的方向
### 为什么图像处理中一般用最大池化而不用平均池化？
池化目的：
1) 保持主要特征不变的同时减小了参数
2) 保持平移、旋转、尺度不变性，增强了神经网络的鲁棒性

而之所以用最大池化而不用平均池化，主要原因是最大池化更能捕捉图像上的变化、梯度的变化，带来更大的局部信息差异化，从而更好地捕捉边缘、纹理等特征。
### softmax loss和cross entropy的区别？
输入数据经过神经网络后得到的logits是一个[n, 1]向量（n表示进行n分类），此时向量中的数字可以是负无穷到正无穷的任意数字，经过softmax函数后才转换为概率。
### 全连接层的作用
全连接层的作用主要是“分类”，将卷积层提取到的特征进行维数调整，同时融合各通道之间的信息。
还有一个作用就是在迁移学习的过程中，有全连接层的模型比没有全连接层的表现更好。
### 训练不收敛（loss不下降）怎么办？
1) 首先检查是否进行了数据归一化
2) 网络的输出层是否用了正确的激活函数（如果最后一层是回归，最好不要用激活函数）
3) 调整学习率试试
4) 仔细检查自己定义的网络层是否有错误
5) 检查loss是否有错误
6) 调整batch_size试试
6) 如果还不行，试试只用几张图片，去除正则化等优化手段，看是否能过拟合。不能的话可能是网络结构有问题，仔细检查代码是否错误，以及调整网络结构试试。
7) 另外：善用tensorboard可视化训练过程
### 怎么计算感受野（receptive field）？
感受野指的是当前层每一个特征对应输入图的区域大小。假设输入特征时5*5特征图（下图中蓝色方块区域），采用kernel_size=3，padding=1, stride=2的方式连续进行两次卷积。第一次卷积后得到3*3特征图（绿色方块区域），再次卷积后得到2*2特征图（橙色方块区域）。下图左列是我们常见的特征图可视化方式，右列是固定大小特征图可视化方式，我们根据固定大小特征图计算感受野
### 详细介绍一下Batch Normalization（怎么计算均值、方差，怎么反向传播等）
神经网络反向传播后每一层的参数都会发生变化，在下一轮正向传播时第l层的输出值$Z_l = W ⋅ A_{l − 1} + b$也会发生变化，从而导致第l层的$A_l = relu ( Z_l )$发生变化。而$A_l$作为第$l + 1$层的输入，$l+1$就需要去适应适应这种数据分布的变化，这就是神经网络难以训练的原因之一。
为此，Batch Normalization的做法是调整数据的分布来改变这一现象，具体做法如下：
训练 -> 反向传播 -> 测试 -> 加在哪里 -> 局限
### RNN和LSTM有什么不同？
**RNN**:（循环神经网络）最主要的特点就是每一步的输出不仅与这一步的输入有关，还跟上一步的输出有关。例如判断一句话中某个词是否是人名，不仅与这个词有关，还与这个词之前的那些单词有关
**LSTM**: 加入了“输入门”，“遗忘门”和“输出门”，解决了简单RNN序列太长梯度消失的问题
### 为什么RNN不用Relu函数而要用tanh函数？
CNN中用ReLU函数能解决梯度消失的问题是因为Relu函数梯度为1，能解决梯度爆炸的问题是因为反向传播时$W_1,W_2 ...W_i$互不相同，它们连乘很大程度上能抵消梯度爆炸的效果；而RNN中用Relu是若干个W连乘，不能解决梯度爆炸的问题。所以想要解决RNN中的梯度消失问题，一般都是用LSTM。
### 写一下卷积的实现

### 空洞卷积的思想？或者说相比如标准卷积有什么改进？
* 标准卷积随着层数加深，虽然感受野会增大，但是分辨率会降低，这样不利于检测小目标；
* 空洞卷积可以在增大感受野的同时保持分辨率不变，有利于提升小目标的检测率，同时能对目标精确定位。
改变空洞卷积中的dilation rate，就能获得多尺度信息。
### CNN模型结构，卷积层的作用
卷积层：提取特征
池化层：：对输入的特征图进行压缩，提取主要特征
全连接层：分类
****